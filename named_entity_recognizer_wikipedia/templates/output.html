{% extends 'layout.html' %}
{% block title %}Output{% endblock %}

{% block content %}

<div class="container">
    <h1 class="text-center my-5">Output</h1>

    <div class="row wp-row">
        <div class="col-md-8 mb-3">
            <h3>
                Text with wiki urls
            </h3>
            <p>
                {% for location, info in output %}
                {% if info[2] == "" %}
                <span>{{ info[0] }}</span>
                {% else %}
                <a class="show-link-preview" href="{{ info[2] }}">{{ info[0] }}</a>
                {% endif %}
                {% endfor %}
            </p>
        </div>
        <div class="col-md-4 mb-3">
            <h3>Download ent file</h3>
            <p class="text-muted">Download the ent-file here. This entfile can be used for calculating the f-scores and
                the cohen-kappa-score with your own script. This can also be done by uploading an
                entfile on the homepage.</p>
            <a href="{{ url_for('download_file', index_user=index_user) }}" style="width: 100%;"
                class="btn btn-primary">Download ent file</a>
        </div>
    </div>

    {% if is_testing == True %}
    <div class="row wp-row">
        <div class="col-md-6 mb-3">
            <h3>Confusion Matrix</h3>
            <img width="100%"
                src="{{ url_for('static', filename='user_files/user_upload_' + index_user + '/plot.png' ) }}" alt="">
        </div>
        <div class="col-md-6 mb-3">
            <h3>Kappa score and f-scores</h3>
            <p>The diagonal line from top left corner to right bottom in the confusion matrix shows the named entities tags that
                were the same for each token in the annotated file and the created file. The more tokens are represented
                on the diagonal line, the better. Besides the confusion matrix, the cohen-kappa-score is calculated,
                which has a value of <b>{{ scores[1] }}</b>. Not only are the named entity tags tested, the wiki urls
                are also evaluated. The evaluation recall score for those is <b>{{ scores[2] }}%</b> and the precision score is <b>{{ scores[3] }}%</b>.
            </p>
            <div style="overflow-x: scroll;">
                <table class="table mt-3">
                    <thead>
                        <tr>
                            <th scope="col">Named entity tag</th>
                            <th scope="col">F-score</th>
                        </tr>
                    </thead>
                    <tbody>
                        {% for f_score in scores[0] %}
                        <tr>
                            <td>{{ scores[4][loop.index0] }}</td>
                            <th style="font-weight: lighter;" scope="row">{{ f_score }}</th>
                        </tr>
                        {% endfor %}
                    </tbody>
                </table>
            </div>
        </div>
    </div>
    {% endif %}

    <div style="overflow-x: scroll;">
        <table class="table mt-3">
            <thead>
                <tr>
                    <th scope="col">Position</th>
                    <th scope="col">Word</th>
                    <th scope="col">Named entity tag</th>
                    <th scope="col">Wiki url</th>
                </tr>
            </thead>
            <tbody>
                {% for location, info in output %}
                {% if info[2] != "" %}
                <tr>
                    <th style="font-weight: lighter;" scope="row">{{ location }}</th>
                    <td>{{ info[0] }}</td>
                    <td style="font-weight: lighter;">{{ info[1] }}</td>
                    <td style="font-weight: lighter;"><a href="{{ info[2] }}">{{ info[2] }}</a></td>
                </tr>
                {% endif %}
                {% endfor %}
            </tbody>
        </table>
    </div>
</div>

{% endblock %}